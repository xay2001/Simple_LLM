# æ¨¡å—åŒ– Transformer è¯­è¨€æ¨¡å‹

åŸºäºé”€å”®æ•™ææ•°æ®è®­ç»ƒçš„å°å‹Transformerè¯­è¨€æ¨¡å‹ï¼Œå®Œå…¨æ¨¡å—åŒ–å®ç°ï¼Œé€‚åˆå­¦ä¹ å’Œå®éªŒã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
21å·/
â”œâ”€â”€ model/                      # ğŸ¯ è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
â”‚   â”œâ”€â”€ transformer_model.pth       # æœ€ç»ˆè®­ç»ƒæ¨¡å‹ (51MB)
â”‚   â””â”€â”€ transformer_model_best.pth  # æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹ (51MB)
â”œâ”€â”€ data/                       # ğŸ“š è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ sales_textbook.txt          # é”€å”®æ•™ææ•°æ® (451KB)
â”‚   â”œâ”€â”€ scifi.txt                   # ç§‘å¹»å°è¯´æ•°æ® (341MB)
â”‚   â””â”€â”€ scifi-finetune.json         # ç§‘å¹»æ•°æ®JSONæ ¼å¼ (575MB)
â”œâ”€â”€ pretrain/                   # ğŸš€ é¢„è®­ç»ƒç›¸å…³ä»£ç 
â”‚   â”œâ”€â”€ model.py                    # Transformeræ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ transformer_model.py        # æ¨¡å‹æ¶æ„å®ç°
â”‚   â””â”€â”€ train.py                    # é¢„è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference/                  # ğŸ² æ¨ç†å’Œæµ‹è¯•
â”‚   â””â”€â”€ example_usage.py            # æ¨ç†æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ finetune/                   # ğŸ”§ å¾®è°ƒç›¸å…³ä»£ç 
â”‚   â””â”€â”€ finetune.py                 # å¾®è°ƒè„šæœ¬
â”œâ”€â”€ results/                    # ğŸ“Š è®­ç»ƒç»“æœå’Œå¯è§†åŒ–
â”‚   â”œâ”€â”€ training_loss.png           # é¢„è®­ç»ƒæŸå¤±æ›²çº¿
â”‚   â””â”€â”€ finetune_curves.png         # å¾®è°ƒæŸå¤±æ›²çº¿
â”œâ”€â”€ requirements.txt            # ğŸ“¦ é¡¹ç›®ä¾èµ–
â””â”€â”€ README.md                   # ğŸ“– é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

##  ğŸš€ å¿«é€Ÿä½¿ç”¨

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 2. é¢„è®­ç»ƒæ¨¡å‹
```bash
cd pretrain
python train.py
```

### 3. å¾®è°ƒæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
```bash
cd finetune
python finetune.py
```

### 4. ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
```bash
cd inference
python example_usage.py
```

##  ğŸ“‹ è¯¦ç»†è¿è¡ŒæŒ‡å—

### ğŸ¯ é¢„è®­ç»ƒé˜¶æ®µ
åœ¨ `pretrain/` ç›®å½•ä¸‹è¿›è¡Œï¼š
- **æ¨¡å‹å®šä¹‰**ï¼š`model.py` å’Œ `transformer_model.py`
- **è®­ç»ƒè„šæœ¬**ï¼š`train.py`
- **è®­ç»ƒæ•°æ®**ï¼šä½¿ç”¨ `data/sales_textbook.txt`

### ğŸ”§ å¾®è°ƒé˜¶æ®µ  
åœ¨ `finetune/` ç›®å½•ä¸‹è¿›è¡Œï¼š
- **å¾®è°ƒè„šæœ¬**ï¼š`finetune.py`
- **å¾®è°ƒæ•°æ®**ï¼šä½¿ç”¨ `data/scifi.txt` æˆ– `data/scifi-finetune.json`

### ğŸ² æ¨ç†æµ‹è¯•
åœ¨ `inference/` ç›®å½•ä¸‹è¿›è¡Œï¼š
- **æ¨ç†è„šæœ¬**ï¼š`example_usage.py`
- **æ¨¡å‹åŠ è½½**ï¼šè‡ªåŠ¨åŠ è½½ `model/` ç›®å½•ä¸‹çš„è®­ç»ƒå¥½çš„æ¨¡å‹

### ğŸ“Š ç»“æœæŸ¥çœ‹
åœ¨ `results/` ç›®å½•ä¸‹æŸ¥çœ‹ï¼š
- **è®­ç»ƒæ›²çº¿**ï¼š`training_loss.png`
- **å¾®è°ƒæ›²çº¿**ï¼š`finetune_curves.png`

##  ğŸ“‚ å„æ–‡ä»¶å¤¹è¯¦ç»†è¯´æ˜

### `model/` - æ¨¡å‹å­˜å‚¨
å­˜æ”¾è®­ç»ƒå®Œæˆçš„æ¨¡å‹æ–‡ä»¶ï¼š
- é¢„è®­ç»ƒåçš„æ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜åˆ°æ­¤ç›®å½•
- æ¨ç†æ—¶ä¼šä»æ­¤ç›®å½•åŠ è½½æ¨¡å‹
- åŒ…å«æœ€ç»ˆæ¨¡å‹å’Œæœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹

### `data/` - æ•°æ®é›†
å­˜æ”¾æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼š
- `sales_textbook.txt`ï¼šé”€å”®æ•™ææ–‡æœ¬ï¼Œç”¨äºé¢„è®­ç»ƒ
- `scifi.txt`ï¼šç§‘å¹»å°è¯´æ–‡æœ¬ï¼Œç”¨äºå¾®è°ƒ
- `scifi-finetune.json`ï¼šJSONæ ¼å¼çš„å¾®è°ƒæ•°æ®

### `pretrain/` - é¢„è®­ç»ƒæ¨¡å—
åŒ…å«é¢„è®­ç»ƒç›¸å…³çš„æ‰€æœ‰ä»£ç ï¼š
- `model.py`ï¼šæ¨¡å‹é…ç½®å’Œæ ¸å¿ƒç»„ä»¶
- `transformer_model.py`ï¼šå®Œæ•´çš„Transformerå®ç°
- `train.py`ï¼šé¢„è®­ç»ƒä¸»è„šæœ¬

### `inference/` - æ¨ç†æ¨¡å—
åŒ…å«æ¨¡å‹ä½¿ç”¨å’Œæµ‹è¯•ä»£ç ï¼š
- `example_usage.py`ï¼šäº¤äº’å¼æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º
- æ”¯æŒå¤šç§ç”Ÿæˆæ¨¡å¼å’Œå‚æ•°è°ƒæ•´

### `finetune/` - å¾®è°ƒæ¨¡å—
åŒ…å«æ¨¡å‹å¾®è°ƒä»£ç ï¼š
- `finetune.py`ï¼šåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒ
- æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†å’Œè¶…å‚æ•°

### `results/` - ç»“æœå¯è§†åŒ–
å­˜æ”¾è®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–ç»“æœï¼š
- è‡ªåŠ¨ç”Ÿæˆçš„æŸå¤±æ›²çº¿å›¾
- å¸®åŠ©åˆ†æè®­ç»ƒæ•ˆæœå’Œè°ƒæ•´å‚æ•°

##  æ¨¡å‹é…ç½®

- å‚æ•°é‡ï¼š~13M
- æ¨¡å‹ç»´åº¦ï¼š64
- æ³¨æ„åŠ›å¤´æ•°ï¼š4
- Transformerå±‚æ•°ï¼š8
- ä¸Šä¸‹æ–‡é•¿åº¦ï¼š16
- æ”¯æŒè®¾å¤‡ï¼šCPU / CUDA / MPS (Apple Silicon)

##  æ¨¡å—åŒ–æ¶æ„

### 1. æ¨¡å‹æ¨¡å— (`model.py`)
```python
from model import Config, TransformerLanguageModel, generate_text

# åˆ›å»ºé…ç½®
config = Config()

# åˆ›å»ºæ¨¡å‹
model = TransformerLanguageModel(
    vocab_size=50000,
    d_model=config.d_model,
    num_heads=config.num_heads,
    num_blocks=config.num_blocks,
    context_length=config.context_length
)

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model, config = TransformerLanguageModel.from_pretrained('model.pth')
```

### 2. è®­ç»ƒæ¨¡å— (`train.py`)
```python
from train import train_model
from model import Config

# è‡ªå®šä¹‰é…ç½®è®­ç»ƒ
config = Config()
config.max_iters = 10000  # æ›´å¤šè®­ç»ƒæ­¥æ•°
model, tokenizer = train_model(config)
```

### 3. æ¨ç†æ¨¡å— (`example_usage.py`)
- ğŸ² äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆ
- ğŸ“¦ æ‰¹é‡ç”Ÿæˆæ¼”ç¤º  
- âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•
- ğŸ¯ å¤šç§ç”Ÿæˆæ¨¡å¼

##  ä½¿ç”¨è¯´æ˜

1. ç¡®ä¿ `sales_textbook.txt` åœ¨é¡¹ç›®ç›®å½•ä¸­
2. è¿è¡Œ `python train.py` è¿›è¡Œè®­ç»ƒ
3. è¿è¡Œ `python example_usage.py` è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ

## ğŸ’» è¿è¡Œå‘½ä»¤è¯¦è§£

### é¢„è®­ç»ƒå‘½ä»¤
```bash
# åŸºç¡€é¢„è®­ç»ƒ
cd pretrain
python train.py

# è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
python train.py --max_iters 10000 --batch_size 32

# æŒ‡å®šGPUè®¾å¤‡
CUDA_VISIBLE_DEVICES=0 python train.py
```

### å¾®è°ƒå‘½ä»¤
```bash
# åŸºç¡€å¾®è°ƒï¼ˆéœ€è¦å…ˆæœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼‰
cd finetune
python finetune.py

# æŒ‡å®šæ¨¡å‹è·¯å¾„å¾®è°ƒ
python finetune.py --model_path ../model/transformer_model_best.pth

# è‡ªå®šä¹‰å¾®è°ƒæ•°æ®
python finetune.py --data_path ../data/custom_data.txt
```

### æ¨ç†å‘½ä»¤
```bash
# äº¤äº’å¼æ¨ç†
cd inference
python example_usage.py

# ä½¿ç”¨ç‰¹å®šæ¨¡å‹æ–‡ä»¶
python example_usage.py --model_path ../model/transformer_model_best.pth

# æ‰¹é‡ç”Ÿæˆæ–‡æœ¬
python example_usage.py --batch_mode --num_samples 10
```

### å…¶ä»–å®ç”¨å‘½ä»¤
```bash
# æŸ¥çœ‹æ¨¡å‹å¤§å°
ls -lh model/

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f pretrain/training.log

# æ¸…ç†ç¼“å­˜æ–‡ä»¶
find . -name "*.pyc" -delete
find . -name "__pycache__" -type d -exec rm -rf {} +
```

### APIä½¿ç”¨ç¤ºä¾‹
```python
from model import TransformerLanguageModel, generate_text
import tiktoken

# åŠ è½½æ¨¡å‹
model, config = TransformerLanguageModel.from_pretrained('transformer_model.pth')
tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")

# ç”Ÿæˆæ–‡æœ¬
text = generate_text(
    model=model,
    tokenizer=tokenizer,
    prompt="Sales techniques",
    max_new_tokens=100,
    temperature=0.8,
    device=config.device
)
print(text)
```

##  åŠŸèƒ½ç‰¹ç‚¹

- **ğŸ”§ å®Œå…¨æ¨¡å—åŒ–**ï¼šæ¨¡å‹ã€è®­ç»ƒã€æ¨ç†åˆ†ç¦»
- **ğŸš€ MPSæ”¯æŒ**ï¼šApple Silicon Mac GPUåŠ é€Ÿ
- **ğŸ² äº¤äº’å¼ç”Ÿæˆ**ï¼šæ”¯æŒå¸¦æç¤ºçš„æ–‡æœ¬ç”Ÿæˆ
- **ğŸ“Š å¯è§†åŒ–è®­ç»ƒ**ï¼šè‡ªåŠ¨ç”ŸæˆæŸå¤±æ›²çº¿
- **âš¡ æ€§èƒ½æµ‹è¯•**ï¼šå†…ç½®ç”Ÿæˆé€Ÿåº¦åŸºå‡†
- **ğŸ’¾ æ¨¡å‹ç®¡ç†**ï¼šä¾¿æ·çš„ä¿å­˜å’ŒåŠ è½½æ¥å£

## âš ï¸ æ³¨æ„äº‹é¡¹

### è¿è¡Œé¡ºåº
1. **é¦–å…ˆé¢„è®­ç»ƒ**ï¼šå¿…é¡»å…ˆè¿è¡Œ `pretrain/train.py` ç”ŸæˆåŸºç¡€æ¨¡å‹
2. **ç„¶åå¾®è°ƒ**ï¼šå¯é€‰æ­¥éª¤ï¼Œä½¿ç”¨ `finetune/finetune.py` è¿›è¡Œé¢†åŸŸé€‚åº”
3. **æœ€åæ¨ç†**ï¼šä½¿ç”¨ `inference/example_usage.py` ç”Ÿæˆæ–‡æœ¬

### ç¯å¢ƒè¦æ±‚
- Python 3.8+
- PyTorch 1.12+
- CUDA (å¯é€‰ï¼ŒGPUåŠ é€Ÿ)
- è‡³å°‘8GBå†…å­˜ï¼ˆå¤§æ•°æ®é›†éœ€è¦æ›´å¤šï¼‰

### å¸¸è§é—®é¢˜

**Q: è®­ç»ƒæ—¶å‡ºç°å†…å­˜ä¸è¶³ï¼Ÿ**
```bash
# å‡å°‘æ‰¹æ¬¡å¤§å°
python train.py --batch_size 16

# æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python train.py --gradient_accumulation_steps 4
```

**Q: æ¨¡å‹æ–‡ä»¶æ‰¾ä¸åˆ°ï¼Ÿ**
```bash
# ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•è¿è¡Œ
pwd  # åº”è¯¥æ˜¾ç¤ºåŒ…å«model/æ–‡ä»¶å¤¹çš„è·¯å¾„
ls model/  # ç¡®è®¤æ¨¡å‹æ–‡ä»¶å­˜åœ¨
```

**Q: æ¨ç†é€Ÿåº¦æ…¢ï¼Ÿ**
```bash
# ä½¿ç”¨GPUåŠ é€Ÿ
export CUDA_VISIBLE_DEVICES=0
python example_usage.py

# æˆ–å‡å°‘ç”Ÿæˆé•¿åº¦
python example_usage.py --max_new_tokens 50
```

## ğŸ› ï¸ å¼€å‘å’Œæ‰©å±•

### è‡ªå®šä¹‰æ¨¡å‹é…ç½®
```python
# åœ¨pretrain/model.pyä¸­ä¿®æ”¹Configç±»
from model import Config

config = Config()
config.d_model = 128      # æ›´å¤§çš„æ¨¡å‹
config.num_heads = 8      # æ›´å¤šæ³¨æ„åŠ›å¤´
config.num_blocks = 12    # æ›´æ·±çš„ç½‘ç»œ
config.max_iters = 10000  # æ›´é•¿è®­ç»ƒ
```

### æ·»åŠ è‡ªå®šä¹‰æ•°æ®
```bash
# 1. å°†æ–°æ•°æ®æ”¾å…¥data/ç›®å½•
cp your_data.txt data/

# 2. ä¿®æ”¹è®­ç»ƒè„šæœ¬ä½¿ç”¨æ–°æ•°æ®
cd pretrain
python train.py --data_path ../data/your_data.txt
```

### è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°
```python
# åœ¨inference/example_usage.pyä¸­è°ƒæ•´
temperature = 0.8    # æ§åˆ¶éšæœºæ€§ (0.1-2.0)
top_k = 50          # å€™é€‰è¯æ•°é‡
max_tokens = 100    # ç”Ÿæˆé•¿åº¦
```

### æ•°æ®æ ¼å¼è¦æ±‚
æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
- **çº¯æ–‡æœ¬**ï¼š`.txt` æ–‡ä»¶ï¼ŒUTF-8ç¼–ç 
- **JSON**ï¼šåŒ…å« `"text"` å­—æ®µçš„JSONæ•°ç»„
- **è‡ªåŠ¨å¤„ç†**ï¼š90%/10% è®­ç»ƒ/éªŒè¯åˆ†å‰²

## ğŸ¯ å¿«é€Ÿå¼€å§‹æ€»ç»“

```bash
# 1. å…‹éš†/ä¸‹è½½é¡¹ç›®åˆ°æœ¬åœ°
# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¿…é¡»æ­¥éª¤ï¼‰
cd pretrain
python train.py

# 4. å¾®è°ƒæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
cd ../finetune
python finetune.py

# 5. æµ‹è¯•æ¨ç†
cd ../inference
python example_usage.py

# 6. æŸ¥çœ‹ç»“æœ
cd ../results
# æŸ¥çœ‹è®­ç»ƒæ›²çº¿å›¾
```