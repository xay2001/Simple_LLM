#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å—åŒ–Transformerè¯­è¨€æ¨¡å‹ - æ¨ç†æ¼”ç¤ºè„šæœ¬

æä¾›äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆå’Œæ‰¹é‡ç”ŸæˆåŠŸèƒ½
"""

import os
import torch
import tiktoken
import sys
from typing import Optional

# æ·»åŠ çˆ¶ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ¨¡å‹ç»„ä»¶
from pretrain.model import TransformerLanguageModel, Config, generate_text

def load_model(model_path: str = '../model/transformer_model.pth') -> tuple[Optional[TransformerLanguageModel], Optional[object], Optional[Config]]:
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        model: åŠ è½½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        config: é…ç½®å¯¹è±¡
    """
    print("=" * 60)
    print("ğŸ”„ åŠ è½½æ¨¡å‹")
    print("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ '{model_path}'")
        print("è¯·å…ˆè¿è¡Œ 'python train.py' è®­ç»ƒæ¨¡å‹")
        return None, None, None
    
    try:
        # åŠ è½½æ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        model, config = TransformerLanguageModel.from_pretrained(model_path)
        
        # è·å–tokenizer
        tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   - æ¨¡å‹å‚æ•°: {model.get_num_params()/1e6:.2f}M")
        print(f"   - æ¨¡å‹ç»´åº¦: {config.d_model}")
        print(f"   - æ³¨æ„åŠ›å¤´æ•°: {config.num_heads}")
        print(f"   - Transformerå±‚æ•°: {config.num_blocks}")
        print(f"   - ä¸Šä¸‹æ–‡é•¿åº¦: {config.context_length}")
        print(f"   - è®¾å¤‡: {config.device}")
        
        return model, tokenizer, config
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

def interactive_generation(model: TransformerLanguageModel, tokenizer, config: Config):
    """äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆ
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        config: é…ç½®å¯¹è±¡
    """
    print("\n" + "=" * 60)
    print("ğŸ² äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆ")
    print("=" * 60)
    print("è¾“å…¥æç¤ºæ–‡æœ¬ï¼Œæ¨¡å‹å°†ç”Ÿæˆç»­å†™å†…å®¹")
    print("å‘½ä»¤:")
    print("  - ç›´æ¥è¾“å…¥æ–‡æœ¬ä½œä¸ºæç¤º")
    print("  - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("  - è¾“å…¥ 'random' è¿›è¡Œéšæœºç”Ÿæˆï¼ˆæ— æç¤ºï¼‰")
    print("  - è¾“å…¥ 'batch' è¿›è¡Œæ‰¹é‡æ¼”ç¤º")
    print("  - è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("-" * 60)
    
    while True:
        try:
            prompt = input("\nğŸ“ è¾“å…¥æç¤º (æˆ–å‘½ä»¤): ").strip()
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            elif prompt.lower() == 'help':
                print("\nå¸®åŠ©ä¿¡æ¯:")
                print("  - è¾“å…¥ä»»ä½•æ–‡æœ¬ä½œä¸ºç”Ÿæˆæç¤º")
                print("  - 'random': éšæœºç”Ÿæˆ")
                print("  - 'batch': æ‰¹é‡æ¼”ç¤º")
                print("  - 'quit'/'exit': é€€å‡ºç¨‹åº")
                continue
            elif prompt.lower() == 'batch':
                batch_generation_demo(model, tokenizer, config)
                continue
            elif prompt.lower() == 'random':
                prompt = ""
                print("ğŸ² éšæœºç”Ÿæˆ...")
            
            # è®¾ç½®ç”Ÿæˆå‚æ•°
            max_tokens = 100
            temperature = 0.8
            
            print(f"\nâš™ï¸ ç”Ÿæˆå‚æ•°: max_tokens={max_tokens}, temperature={temperature}")
            print("ğŸš€ ç”Ÿæˆä¸­...")
            print("-" * 50)
            
            # ç”Ÿæˆæ–‡æœ¬
            generated_text = generate_text(
                model=model,
                tokenizer=tokenizer,
                prompt=prompt,
                max_new_tokens=max_tokens,
                temperature=temperature,
                device=config.device
            )
            
            # æ˜¾ç¤ºç»“æœ
            if prompt:
                print(f"ğŸ“¥ åŸå§‹æç¤º: {prompt}")
                if generated_text.startswith(prompt):
                    new_text = generated_text[len(prompt):].strip()
                    print(f"âœ¨ ç”Ÿæˆå†…å®¹: {new_text}")
                else:
                    print(f"âœ¨ å®Œæ•´ç”Ÿæˆ: {generated_text}")
            else:
                print(f"âœ¨ éšæœºç”Ÿæˆ: {generated_text}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
            break
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")

def batch_generation_demo(model: TransformerLanguageModel, tokenizer, config: Config):
    """æ‰¹é‡ç”Ÿæˆæ¼”ç¤º
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        config: é…ç½®å¯¹è±¡
    """
    print("\n" + "=" * 60)
    print("ğŸ“¦ æ‰¹é‡ç”Ÿæˆæ¼”ç¤º")
    print("=" * 60)
    
    # é¢„å®šä¹‰çš„æç¤ºè¯
    prompts = [
        "",  # éšæœºç”Ÿæˆ
        "Sales",
        "Building rapport",
        "Customer service",
        "Communication skills",
        "The importance of",
        "In business",
        "Effective communication",
        "Understanding customers",
        "Building relationships"
    ]
    
    # ä¸åŒçš„æ¸©åº¦è®¾ç½®
    temperatures = [0.5, 0.8, 1.2]
    
    print(f"å°†ä¸º {len(prompts)} ä¸ªæç¤ºå„ç”Ÿæˆ {len(temperatures)} ä¸ªç‰ˆæœ¬")
    print("ä½¿ç”¨ä¸åŒçš„æ¸©åº¦å‚æ•°æ§åˆ¶åˆ›é€ æ€§\n")
    
    for i, prompt in enumerate(prompts, 1):
        prompt_display = f"'{prompt}'" if prompt else "'éšæœºç”Ÿæˆ'"
        print(f"\n{i}. æç¤º: {prompt_display}")
        print("=" * 50)
        
        for j, temp in enumerate(temperatures, 1):
            print(f"\n  ğŸ“Š ç‰ˆæœ¬ {j} (æ¸©åº¦={temp}):")
            print("  " + "-" * 45)
            
            try:
                generated_text = generate_text(
                    model=model,
                    tokenizer=tokenizer,
                    prompt=prompt,
                    max_new_tokens=50,  # è¾ƒçŸ­çš„ç”Ÿæˆç”¨äºæ¼”ç¤º
                    temperature=temp,
                    device=config.device
                )
                
                # å¤„ç†æ˜¾ç¤ºæ–‡æœ¬
                if prompt and generated_text.startswith(prompt):
                    display_text = generated_text[len(prompt):].strip()
                else:
                    display_text = generated_text
                
                # æ ¼å¼åŒ–è¾“å‡º
                words = display_text.split()
                lines = []
                current_line = "  "
                
                for word in words:
                    if len(current_line + word) > 70:  # æ§åˆ¶è¡Œé•¿åº¦
                        lines.append(current_line.rstrip())
                        current_line = "  " + word + " "
                    else:
                        current_line += word + " "
                
                if current_line.strip():
                    lines.append(current_line.rstrip())
                
                for line in lines:
                    print(line)
                
            except Exception as e:
                print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")

def benchmark_generation(model: TransformerLanguageModel, tokenizer, config: Config):
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        config: é…ç½®å¯¹è±¡
    """
    print("\n" + "=" * 60)
    print("âš¡ ç”Ÿæˆæ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    import time
    
    test_prompts = ["Sales", "Customer", "Building"]
    test_lengths = [50, 100, 200]
    
    print(f"æµ‹è¯•é…ç½®:")
    print(f"  - æç¤ºæ•°é‡: {len(test_prompts)}")
    print(f"  - ç”Ÿæˆé•¿åº¦: {test_lengths}")
    print(f"  - è®¾å¤‡: {config.device}")
    print()
    
    for length in test_lengths:
        print(f"ğŸ“ æµ‹è¯•é•¿åº¦: {length} tokens")
        total_time = 0
        total_tokens = 0
        
        for prompt in test_prompts:
            start_time = time.time()
            
            generated_text = generate_text(
                model=model,
                tokenizer=tokenizer,
                prompt=prompt,
                max_new_tokens=length,
                temperature=1.0,
                device=config.device
            )
            
            end_time = time.time()
            generation_time = end_time - start_time
            tokens_generated = len(tokenizer.encode(generated_text)) - len(tokenizer.encode(prompt))
            
            total_time += generation_time
            total_tokens += tokens_generated
            
            print(f"  {prompt:15s}: {generation_time:.2f}s, {tokens_generated} tokens, {tokens_generated/generation_time:.1f} tokens/s")
        
        avg_speed = total_tokens / total_time
        print(f"  å¹³å‡é€Ÿåº¦: {avg_speed:.1f} tokens/s\n")

def main():
    """ä¸»å‡½æ•°"""
    print("æ¨¡å—åŒ–Transformerè¯­è¨€æ¨¡å‹ - æ¨ç†æ¼”ç¤º")
    print("ç‰ˆæœ¬: 2.0 (æ¨¡å—åŒ–)")
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    model_path = '../model/transformer_model.pth'
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer, config = load_model(model_path)
    
    if model is None:
        print("\nâŒ æ— æ³•åŠ è½½æ¨¡å‹ï¼Œç¨‹åºé€€å‡º")
        return
    
    print("\nğŸ¯ é€‰æ‹©æ“ä½œæ¨¡å¼:")
    print("1. äº¤äº’å¼ç”Ÿæˆ (é»˜è®¤)")
    print("2. æ‰¹é‡æ¼”ç¤º")
    print("3. æ€§èƒ½æµ‹è¯•")
    print("4. å…¨éƒ¨è¿è¡Œ")
    
    try:
        choice = input("\né€‰æ‹©æ¨¡å¼ (1-4, é»˜è®¤ä¸º1): ").strip()
        
        if choice == '2':
            batch_generation_demo(model, tokenizer, config)
        elif choice == '3':
            benchmark_generation(model, tokenizer, config)
        elif choice == '4':
            batch_generation_demo(model, tokenizer, config)
            benchmark_generation(model, tokenizer, config)
            interactive_generation(model, tokenizer, config)
        else:  # é»˜è®¤ä¸ºäº¤äº’å¼
            interactive_generation(model, tokenizer, config)
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºé”™è¯¯: {e}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ„Ÿè°¢ä½¿ç”¨æ¨¡å—åŒ–Transformerè¯­è¨€æ¨¡å‹ï¼")
    print("=" * 60)

if __name__ == "__main__":
    main() 