#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Transformerè¯­è¨€æ¨¡å‹å¾®è°ƒè„šæœ¬

ä½¿ç”¨ç§‘å¹»å°è¯´æ•°æ®é›†å¯¹é¢„è®­ç»ƒçš„Transformeræ¨¡å‹è¿›è¡Œå¾®è°ƒ
"""

import os
import sys
import json
import torch
import torch.nn as nn
import tiktoken
import numpy as np
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import warnings
from typing import Tuple, Optional, List, Dict
import time
from tqdm import tqdm
import argparse

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å‹
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'pretrain'))

# å¯¼å…¥æ¨¡å‹ç»„ä»¶
from model import Config, TransformerLanguageModel, generate_text

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)

class FinetuneConfig(Config):
    """å¾®è°ƒä¸“ç”¨é…ç½®ç±»ï¼Œç»§æ‰¿é¢„è®­ç»ƒé…ç½®"""
    def __init__(self):
        super().__init__()
        # å¾®è°ƒä¸“ç”¨å‚æ•°
        self.max_iters = 1000
        self.learning_rate = 5e-5  # å¾®è°ƒé€šå¸¸ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
        self.eval_interval = 100
        self.eval_iters = 50
        self.warmup_steps = 100
        self.weight_decay = 0.01
        self.save_interval = 200
        
        # æ•°æ®è·¯å¾„
        self.pretrained_model_path = '../model/transformer_model.pth'
        self.finetune_data_path = '../data/scifi-finetune.json'
        self.output_model_path = '../model/transformer_model_finetuned.pth'
        
        print(f"ğŸ”§ å¾®è°ƒé…ç½®:")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"   - ä¸Šä¸‹æ–‡é•¿åº¦: {self.context_length}")
        print(f"   - å­¦ä¹ ç‡: {self.learning_rate}")
        print(f"   - æœ€å¤§è¿­ä»£: {self.max_iters}")

class FinetuneDataset(Dataset):
    """å¾®è°ƒæ•°æ®é›†ç±»
    
    å¤„ç†instruction-input-outputæ ¼å¼çš„æ•°æ®
    """
    def __init__(self, data: List[Dict], tokenizer, max_length: int):
        """
        Args:
            data: åŒ…å«instruction, input, outputçš„å­—å…¸åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: åºåˆ—æœ€å¤§é•¿åº¦
        """
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.processed_data = self._process_data()
        
    def _process_data(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """é¢„å¤„ç†æ•°æ®ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºtokenåºåˆ—"""
        processed = []
        
        print("ğŸ”„ é¢„å¤„ç†å¾®è°ƒæ•°æ®...")
        for item in tqdm(self.data, desc="å¤„ç†æ•°æ®", ncols=80, leave=False):
            # æ„é€ è®­ç»ƒæ–‡æœ¬æ ¼å¼
            if item.get('input', '').strip():
                # æœ‰inputçš„æƒ…å†µ
                text = f"æŒ‡ä»¤ï¼š{item['instruction']}\nè¾“å…¥ï¼š{item['input']}\nè¾“å‡ºï¼š{item['output']}"
            else:
                # æ²¡æœ‰inputçš„æƒ…å†µ
                text = f"æŒ‡ä»¤ï¼š{item['instruction']}\nè¾“å‡ºï¼š{item['output']}"
            
            # å¯¹æ–‡æœ¬è¿›è¡Œtokenization
            tokens = self.tokenizer.encode(text)
            
            # å¦‚æœåºåˆ—å¤ªçŸ­ï¼Œè·³è¿‡
            if len(tokens) < 2:
                continue
                
            # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œæˆªæ–­
            if len(tokens) > self.max_length + 1:
                tokens = tokens[:self.max_length + 1]
            
            # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡åºåˆ—
            if len(tokens) > 1:
                input_tokens = tokens[:-1]
                target_tokens = tokens[1:]
                
                # å¡«å……åˆ°å›ºå®šé•¿åº¦
                while len(input_tokens) < self.max_length:
                    input_tokens.append(self.tokenizer.eot_token)  # ä½¿ç”¨ç»“æŸtokenå¡«å……
                    target_tokens.append(self.tokenizer.eot_token)
                
                x = torch.tensor(input_tokens[:self.max_length], dtype=torch.long)
                y = torch.tensor(target_tokens[:self.max_length], dtype=torch.long)
                processed.append((x, y))
        
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°: {len(processed)}")
        return processed
        
    def __len__(self) -> int:
        return len(self.processed_data)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.processed_data[idx]

def load_finetune_data(data_path: str) -> Tuple[Optional[List[Dict]], Optional[object]]:
    """åŠ è½½å¾®è°ƒæ•°æ®
    
    Args:
        data_path: JSONæ•°æ®æ–‡ä»¶è·¯å¾„
    
    Returns:
        data: æ•°æ®åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨å¯¹è±¡
    """
    print("=" * 60)
    print("ğŸ“š åŠ è½½å¾®è°ƒæ•°æ®")
    print("=" * 60)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_path):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ '{data_path}'")
        return None, None
    
    # è¯»å–JSONæ•°æ®
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
        return None, None
    
    # éªŒè¯æ•°æ®æ ¼å¼
    if not isinstance(data, list):
        print("âŒ æ•°æ®æ ¼å¼é”™è¯¯ï¼šåº”è¯¥æ˜¯åŒ…å«å­—å…¸çš„åˆ—è¡¨")
        return None, None
    
    # æ£€æŸ¥æ•°æ®æ¡ç›®æ ¼å¼
    valid_data = []
    for i, item in enumerate(data):
        if not isinstance(item, dict):
            print(f"âš ï¸ è·³è¿‡ç¬¬{i+1}æ¡æ•°æ®ï¼šä¸æ˜¯å­—å…¸æ ¼å¼")
            continue
        
        if 'instruction' not in item or 'output' not in item:
            print(f"âš ï¸ è·³è¿‡ç¬¬{i+1}æ¡æ•°æ®ï¼šç¼ºå°‘å¿…è¦å­—æ®µ")
            continue
            
        valid_data.append(item)
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   - æ€»æ¡ç›®æ•°: {len(data)}")
    print(f"   - æœ‰æ•ˆæ¡ç›®æ•°: {len(valid_data)}")
    
    if len(valid_data) > 0:
        # æ˜¾ç¤ºæ•°æ®æ ·ä¾‹
        sample = valid_data[0]
        print(f"   - æ•°æ®æ ·ä¾‹:")
        print(f"     æŒ‡ä»¤: {sample['instruction'][:50]}...")
        if sample.get('input'):
            print(f"     è¾“å…¥: {sample['input'][:50]}...")
        print(f"     è¾“å‡º: {sample['output'][:50]}...")
    
    # åˆå§‹åŒ–tokenizer
    try:
        tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")
        print(f"   - è¯æ±‡è¡¨å¤§å°: {tokenizer.n_vocab:,}")
    except Exception as e:
        print(f"âŒ Tokenizeråˆå§‹åŒ–å¤±è´¥: {e}")
        return None, None
    
    return valid_data, tokenizer

def load_pretrained_model(model_path: str, tokenizer, config: FinetuneConfig) -> Optional[TransformerLanguageModel]:
    """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        tokenizer: åˆ†è¯å™¨
        config: é…ç½®å¯¹è±¡
    
    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    print("\nğŸ”„ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ '{model_path}'")
        return None
    
    try:
        # åŠ è½½checkpoint
        checkpoint = torch.load(model_path, map_location=config.device)
        
        # æ£€æŸ¥checkpointæ ¼å¼å¹¶æå–ä¿¡æ¯
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            # å®Œæ•´çš„checkpointæ ¼å¼
            state_dict = checkpoint['model_state_dict']
            saved_config = checkpoint.get('config', {})
            saved_vocab_size = checkpoint.get('vocab_size', tokenizer.n_vocab)
            
            print(f"âœ… æ£€æµ‹åˆ°å®Œæ•´checkpointæ ¼å¼")
            print(f"   - ä¿å­˜çš„è¯æ±‡è¡¨å¤§å°: {saved_vocab_size}")
            print(f"   - ä¿å­˜çš„é…ç½®: {saved_config}")
            
            # æ›´æ–°é…ç½®ä»¥åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹
            if 'd_model' in saved_config:
                config.d_model = saved_config['d_model']
            if 'num_heads' in saved_config:
                config.num_heads = saved_config['num_heads']
            if 'num_blocks' in saved_config:
                config.num_blocks = saved_config['num_blocks']
            if 'context_length' in saved_config:
                config.context_length = saved_config['context_length']
            if 'dropout' in saved_config:
                config.dropout = saved_config['dropout']
                
            # ä½¿ç”¨ä¿å­˜çš„è¯æ±‡è¡¨å¤§å°
            vocab_size = saved_vocab_size
            
        else:
            # ç›´æ¥çš„çŠ¶æ€å­—å…¸æ ¼å¼
            state_dict = checkpoint
            vocab_size = tokenizer.n_vocab
            print("âœ… æ£€æµ‹åˆ°çŠ¶æ€å­—å…¸æ ¼å¼")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = TransformerLanguageModel(
            vocab_size=vocab_size,
            d_model=config.d_model,
            num_heads=config.num_heads,
            num_blocks=config.num_blocks,
            context_length=config.context_length,
            dropout=config.dropout
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        model.load_state_dict(state_dict)
        model.to(config.device)
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   - æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

@torch.no_grad()
def estimate_loss(model: TransformerLanguageModel, train_loader: DataLoader, 
                 val_loader: DataLoader, eval_iters: int, device: str) -> dict:
    """ä¼°ç®—è®­ç»ƒå’ŒéªŒè¯æŸå¤±"""
    out = {}
    model.eval()
    
    for split, dataloader in [('train', train_loader), ('val', val_loader)]:
        if dataloader is None:
            out[split] = float('inf')
            continue
            
        losses = []
        count = 0
        
        for xb, yb in dataloader:
            if count >= eval_iters:
                break
            
            xb, yb = xb.to(device), yb.to(device)
            try:
                logits, loss = model(xb, yb)
                losses.append(loss.item())
                count += 1
            except Exception as e:
                print(f"âš ï¸ è¯„ä¼°æ—¶å‡ºç°é”™è¯¯: {e}")
                continue
        
        if losses:
            out[split] = np.mean(losses)
        else:
            out[split] = float('inf')
    
    model.train()
    return out

def create_optimizer(model: TransformerLanguageModel, config: FinetuneConfig):
    """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    # ä¸ºä¸åŒçš„å‚æ•°ç»„è®¾ç½®ä¸åŒçš„æƒé‡è¡°å‡
    decay_params = []
    no_decay_params = []
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            if 'weight' in name and 'embedding' not in name:
                decay_params.append(param)
            else:
                no_decay_params.append(param)
    
    optimizer_groups = [
        {'params': decay_params, 'weight_decay': config.weight_decay},
        {'params': no_decay_params, 'weight_decay': 0.0}
    ]
    
    optimizer = torch.optim.AdamW(optimizer_groups, lr=config.learning_rate)
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    def lr_lambda(step):
        if step < config.warmup_steps:
            return step / config.warmup_steps
        else:
            progress = (step - config.warmup_steps) / (config.max_iters - config.warmup_steps)
            return max(0.1, 0.5 * (1 + np.cos(np.pi * progress)))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    return optimizer, scheduler

def train_model(config: FinetuneConfig) -> Tuple[Optional[TransformerLanguageModel], Optional[object]]:
    """å¾®è°ƒæ¨¡å‹ä¸»å‡½æ•°"""
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹å¾®è°ƒTransformerè¯­è¨€æ¨¡å‹")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    data, tokenizer = load_finetune_data(config.finetune_data_path)
    if data is None or tokenizer is None:
        return None, None
    
    # 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = load_pretrained_model(config.pretrained_model_path, tokenizer, config)
    if model is None:
        return None, None
    
    # 3. æ•°æ®åˆ†å‰²
    split_idx = int(0.9 * len(data))
    train_data = data[:split_idx]
    val_data = data[split_idx:] if len(data) > split_idx else None
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"   - è®­ç»ƒæ ·æœ¬: {len(train_data)}")
    print(f"   - éªŒè¯æ ·æœ¬: {len(val_data) if val_data else 0}")
    
    # 4. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = FinetuneDataset(train_data, tokenizer, config.context_length)
    val_dataset = FinetuneDataset(val_data, tokenizer, config.context_length) if val_data else None
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.batch_size, 
        shuffle=True,
        num_workers=0,
        pin_memory=True if config.device != 'cpu' else False
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config.batch_size, 
        shuffle=False,
        num_workers=0,
        pin_memory=True if config.device != 'cpu' else False
    ) if val_dataset else None
    
    # 5. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer, scheduler = create_optimizer(model, config)
    
    # 6. è®­ç»ƒå¾ªç¯
    print("\nğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    
    # åˆå§‹è¯„ä¼°
    print("\nğŸ“Š åˆå§‹æ¨¡å‹è¯„ä¼°...")
    losses = estimate_loss(model, train_loader, val_loader, config.eval_iters, config.device)
    print(f"åˆå§‹æŸå¤± - è®­ç»ƒ: {losses['train']:.4f}, éªŒè¯: {losses['val']:.4f}")
    
    # è®­ç»ƒè¿›åº¦æ¡
    progress_bar = tqdm(range(config.max_iters), desc="å¾®è°ƒè¿›åº¦", 
                       ncols=100, dynamic_ncols=True, leave=True)
    
    model.train()
    train_iter = iter(train_loader)
    
    for iter_num in progress_bar:
        try:
            # è·å–æ‰¹æ¬¡æ•°æ®
            try:
                xb, yb = next(train_iter)
            except StopIteration:
                train_iter = iter(train_loader)  # é‡æ–°å¼€å§‹
                xb, yb = next(train_iter)
            
            xb, yb = xb.to(config.device), yb.to(config.device)
            
            # å‰å‘ä¼ æ’­
            logits, loss = model(xb, yb)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            # æ›´æ–°å‚æ•°
            optimizer.step()
            scheduler.step()
            
            # è®°å½•æŸå¤±
            train_losses.append(loss.item())
            
            # æ›´æ–°è¿›åº¦æ¡
            current_lr = scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                'æŸå¤±': f'{loss.item():.4f}',
                'å­¦ä¹ ç‡': f'{current_lr:.2e}'
            })
            
            # å®šæœŸè¯„ä¼°
            if (iter_num + 1) % config.eval_interval == 0:
                print(f"\nğŸ“Š ç¬¬ {iter_num + 1} è½®è¯„ä¼°...")
                losses = estimate_loss(model, train_loader, val_loader, config.eval_iters, config.device)
                val_losses.append(losses['val'])
                
                print(f"æŸå¤± - è®­ç»ƒ: {losses['train']:.4f}, éªŒè¯: {losses['val']:.4f}")
                
                # æ›´æ–°æœ€ä½³éªŒè¯æŸå¤±ï¼ˆä½†ä¸ä¿å­˜ï¼‰
                if losses['val'] < best_val_loss:
                    best_val_loss = losses['val']
                    print(f"ğŸ¯ å‘ç°æ›´å¥½æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.4f}")
                
                # ç”Ÿæˆæ ·ä¾‹æ–‡æœ¬
                print("\nğŸ“ ç”Ÿæˆæ ·ä¾‹:")
                sample_text = generate_text(
                    model, tokenizer, 
                    prompt="æŒ‡ä»¤ï¼šè¯·ç»­å†™è¿™ä¸ªç§‘å¹»æ•…äº‹\nè¾“å…¥ï¼šåœ¨é¥è¿œçš„æœªæ¥\nè¾“å‡ºï¼š", 
                    max_new_tokens=50,
                    device=config.device
                )
                print(f"   {sample_text}")
            
            # ç§»é™¤æ£€æŸ¥ç‚¹ä¿å­˜ï¼Œåªåœ¨æœ€åä¿å­˜æœ€ç»ˆæ¨¡å‹
                
        except Exception as e:
            print(f"\nâš ï¸ è®­ç»ƒæ­¥éª¤ {iter_num} å‡ºç°é”™è¯¯: {e}")
            continue
    
    progress_bar.close()
    
    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {config.output_model_path}")
    
    # ä¿å­˜å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯ï¼ˆåŒ…æ‹¬é…ç½®ï¼‰
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': {
            'd_model': config.d_model,
            'num_heads': config.num_heads,
            'num_blocks': config.num_blocks,
            'context_length': config.context_length,
            'dropout': config.dropout
        },
        'vocab_size': model.vocab_size,
        'best_val_loss': best_val_loss,
        'final_train_loss': train_losses[-1] if train_losses else None
    }, config.output_model_path)
    
    # 8. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if len(val_losses) > 0:
        plot_training_curves(train_losses, val_losses, config)
    
    print("\nğŸ‰ å¾®è°ƒå®Œæˆ!")
    print(f"   - æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"   - æ¨¡å‹ä¿å­˜è·¯å¾„: {config.output_model_path}")
    
    return model, tokenizer

def plot_training_curves(train_losses: List[float], val_losses: List[float], config: FinetuneConfig):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    try:
        plt.figure(figsize=(12, 4))
        
        # è®­ç»ƒæŸå¤±
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, alpha=0.7, label='è®­ç»ƒæŸå¤±')
        plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')
        plt.xlabel('è¿­ä»£æ­¥æ•°')
        plt.ylabel('æŸå¤±å€¼')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # éªŒè¯æŸå¤±
        plt.subplot(1, 2, 2)
        eval_steps = [(i + 1) * config.eval_interval for i in range(len(val_losses))]
        plt.plot(eval_steps, val_losses, 'o-', label='éªŒè¯æŸå¤±', color='orange')
        plt.title('éªŒè¯æŸå¤±æ›²çº¿')
        plt.xlabel('è¿­ä»£æ­¥æ•°')
        plt.ylabel('æŸå¤±å€¼')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('../results/finetune_curves.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° ../results/finetune_curves.png")
        
    except Exception as e:
        print(f"âš ï¸ ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")

def demo_generation(model: TransformerLanguageModel, tokenizer, config: FinetuneConfig):
    """æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("ğŸ“ æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("=" * 60)
    
    prompts = [
        "æŒ‡ä»¤ï¼šç»­å†™ç§‘å¹»å°è¯´\nè¾“å…¥ï¼šå¤ªç©ºä¸­çš„æˆ˜èˆ°\nè¾“å‡ºï¼š",
        "æŒ‡ä»¤ï¼šæè¿°æœªæ¥ä¸–ç•Œ\nè¾“å‡ºï¼š",
        "æŒ‡ä»¤ï¼šåˆ›ä½œæœºå™¨äººæ•…äº‹\nè¾“å…¥ï¼šæ™ºèƒ½æœºå™¨äººè§‰é†’\nè¾“å‡ºï¼š",
    ]
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\nğŸ”® ç¤ºä¾‹ {i}:")
        print(f"è¾“å…¥: {prompt}")
        
        generated = generate_text(
            model, tokenizer, 
            prompt=prompt, 
            max_new_tokens=100,
            device=config.device
        )
        print(f"ç”Ÿæˆ: {generated}")
        print("-" * 60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¾®è°ƒTransformerè¯­è¨€æ¨¡å‹')
    parser.add_argument('--data_path', type=str, default='../data/scifi-finetune.json',
                       help='å¾®è°ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--model_path', type=str, default='../model/transformer_model.pth',
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    parser.add_argument('--output_path', type=str, default='../model/transformer_model_finetuned.pth',
                       help='è¾“å‡ºæ¨¡å‹è·¯å¾„')
    parser.add_argument('--max_iters', type=int, default=1000,
                       help='æœ€å¤§è®­ç»ƒè¿­ä»£æ•°')
    parser.add_argument('--learning_rate', type=float, default=5e-5,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--batch_size', type=int, default=4,
                       help='æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = FinetuneConfig()
    config.finetune_data_path = args.data_path
    config.pretrained_model_path = args.model_path
    config.output_model_path = args.output_path
    config.max_iters = args.max_iters
    config.learning_rate = args.learning_rate
    config.batch_size = args.batch_size
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs('../results', exist_ok=True)
    os.makedirs(os.path.dirname(config.output_model_path), exist_ok=True)
    
    # å¼€å§‹å¾®è°ƒ
    model, tokenizer = train_model(config)
    
    if model is not None and tokenizer is not None:
        # æ¼”ç¤ºç”Ÿæˆ
        demo_generation(model, tokenizer, config)
    else:
        print("âŒ å¾®è°ƒå¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main() 