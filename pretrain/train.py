#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å—åŒ–Transformerè¯­è¨€æ¨¡å‹ - è®­ç»ƒè„šæœ¬

åŒ…å«æ•°æ®åŠ è½½ã€è®­ç»ƒå¾ªç¯å’Œæ¨¡å‹ä¿å­˜é€»è¾‘
"""

import os
import torch
import torch.nn as nn
import tiktoken
import numpy as np
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import warnings
from typing import Tuple, Optional
import time
from tqdm import tqdm

# å¯¼å…¥æ¨¡å‹ç»„ä»¶
from model import Config, TransformerLanguageModel, generate_text

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)

class TextDataset(Dataset):
    """æ–‡æœ¬æ•°æ®é›†ç±»
    
    å°†é•¿æ–‡æœ¬åˆ‡åˆ†æˆå›ºå®šé•¿åº¦çš„åºåˆ—ï¼Œç”¨äºè¯­è¨€å»ºæ¨¡è®­ç»ƒ
    """
    def __init__(self, data: list, tokenizer, max_length: int):
        """
        Args:
            data: tokenåŒ–åçš„æ•°æ®åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: åºåˆ—æœ€å¤§é•¿åº¦
        """
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self) -> int:
        return len(self.data) - self.max_length
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
        
        Returns:
            x: è¾“å…¥åºåˆ— (max_length,)
            y: ç›®æ ‡åºåˆ— (max_length,) - è¾“å…¥åºåˆ—å‘å³åç§»ä¸€ä½
        """
        chunk = self.data[idx:idx + self.max_length + 1]
        x = torch.tensor(chunk[:-1], dtype=torch.long)
        y = torch.tensor(chunk[1:], dtype=torch.long)
        return x, y

def load_data(data_file: str = '../data/scifi.txt') -> Tuple[Optional[list], Optional[list], Optional[object]]:
    """åŠ è½½å’Œé¢„å¤„ç†æ–‡æœ¬æ•°æ®
    
    Args:
        data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
    
    Returns:
        train_data: è®­ç»ƒæ•°æ®tokenåˆ—è¡¨
        val_data: éªŒè¯æ•°æ®tokenåˆ—è¡¨ 
        tokenizer: åˆ†è¯å™¨å¯¹è±¡
    """
    print("=" * 60)
    print("ğŸ“š æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")
    print("=" * 60)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ '{data_file}'")
        print("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶åœ¨æŒ‡å®šè·¯å¾„ä¸­")
        return None, None, None
    
    # è¯»å–æ–‡æœ¬æ•°æ®
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {data_file}")
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None, None, None
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   - æ€»å­—ç¬¦æ•°: {len(text):,}")
    print(f"   - é¢„è§ˆ: {text[:100]}...")
    
    # ä½¿ç”¨GPT-3å…¼å®¹çš„tokenizer
    print("\næ­£åœ¨è¿›è¡ŒTokenåŒ–...")
    try:
        tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")
        tokens = tokenizer.encode(text)
    except Exception as e:
        print(f"âŒ TokenåŒ–å¤±è´¥: {e}")
        return None, None, None
    
    print(f"âœ… TokenåŒ–å®Œæˆ")
    print(f"   - æ€»tokenæ•°: {len(tokens):,}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {tokenizer.n_vocab:,}")
    print(f"   - å‹ç¼©æ¯”: {len(text)/len(tokens):.2f} å­—ç¬¦/token")
    
    # æ•°æ®åˆ†å‰²ï¼š90%è®­ç»ƒï¼Œ10%éªŒè¯
    split_idx = int(0.9 * len(tokens))
    train_data = tokens[:split_idx]
    val_data = tokens[split_idx:]
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"   - è®­ç»ƒæ•°æ®: {len(train_data):,} tokens ({len(train_data)/len(tokens)*100:.1f}%)")
    print(f"   - éªŒè¯æ•°æ®: {len(val_data):,} tokens ({len(val_data)/len(tokens)*100:.1f}%)")
    
    return train_data, val_data, tokenizer

@torch.no_grad()
def estimate_loss(model: TransformerLanguageModel, train_loader: DataLoader, 
                 val_loader: DataLoader, eval_iters: int, device: str) -> dict:
    """ä¼°ç®—è®­ç»ƒå’ŒéªŒè¯æŸå¤±
    
    Args:
        model: è¦è¯„ä¼°çš„æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        eval_iters: è¯„ä¼°è¿­ä»£æ¬¡æ•°
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        åŒ…å«è®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„å­—å…¸
    """
    out = {}
    model.eval()
    
    for split, dataloader in [('train', train_loader), ('val', val_loader)]:
        losses = torch.zeros(eval_iters)
        
        # æ·»åŠ è¯„ä¼°è¿›åº¦æ¡
        desc = f"è¯„ä¼°{split}æŸå¤±"
        eval_pbar = tqdm(enumerate(dataloader), desc=desc, total=eval_iters, 
                        leave=False, ncols=100, unit="batch")
        
        for k, (xb, yb) in eval_pbar:
            if k >= eval_iters:
                break
            
            xb, yb = xb.to(device), yb.to(device)
            logits, loss = model(xb, yb)
            losses[k] = loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡
            eval_pbar.set_postfix({'æŸå¤±': f'{loss.item():.4f}'})
        
        eval_pbar.close()
        out[split] = losses.mean()
    
    model.train()
    return out

def create_data_loaders(train_data: list, val_data: list, tokenizer, config: Config) -> Tuple[DataLoader, DataLoader]:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨
    
    Args:
        train_data: è®­ç»ƒæ•°æ®
        val_data: éªŒè¯æ•°æ®
        tokenizer: åˆ†è¯å™¨
        config: é…ç½®å¯¹è±¡
    
    Returns:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
    """
    print("\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TextDataset(train_data, tokenizer, config.context_length)
    val_dataset = TextDataset(val_data, tokenizer, config.context_length)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.batch_size, 
        shuffle=True,
        num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        pin_memory=True if config.device != 'cpu' else False
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config.batch_size, 
        shuffle=False,
        num_workers=0,
        pin_memory=True if config.device != 'cpu' else False
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    print(f"   - è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"   - éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"   - åºåˆ—é•¿åº¦: {config.context_length}")
    
    return train_loader, val_loader

def train_model(config: Config, data_file: str = '../data/scifi.txt', 
               save_path: str = '../model/transformer_model.pth') -> Tuple[Optional[TransformerLanguageModel], Optional[object]]:
    """è®­ç»ƒæ¨¡å‹ä¸»å‡½æ•°
    
    Args:
        config: æ¨¡å‹é…ç½®
        data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    
    Returns:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
    """
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒTransformerè¯­è¨€æ¨¡å‹")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    train_data, val_data, tokenizer = load_data(data_file)
    if train_data is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè®­ç»ƒä¸­æ­¢")
        return None, None
    
    vocab_size = tokenizer.n_vocab
    
    # æ˜¾ç¤ºè®­ç»ƒé…ç½®
    print(f"\nâš™ï¸ è®­ç»ƒé…ç½®:")
    print(f"   - è®¾å¤‡: {config.device}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {vocab_size:,}")
    print(f"   - æ¨¡å‹ç»´åº¦: {config.d_model}")
    print(f"   - æ³¨æ„åŠ›å¤´æ•°: {config.num_heads}")
    print(f"   - Transformerå±‚æ•°: {config.num_blocks}")
    print(f"   - ä¸Šä¸‹æ–‡é•¿åº¦: {config.context_length}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"   - å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"   - è®­ç»ƒæ­¥æ•°: {config.max_iters}")
    print(f"   - Dropout: {config.dropout}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_data_loaders(train_data, val_data, tokenizer, config)
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
    model = TransformerLanguageModel(
        vocab_size=vocab_size,
        d_model=config.d_model,
        num_heads=config.num_heads,
        num_blocks=config.num_blocks,
        context_length=config.context_length,
        dropout=config.dropout
    ).to(config.device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=config.learning_rate,
        weight_decay=1e-4  # æ·»åŠ æƒé‡è¡°å‡
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.max_iters, eta_min=config.learning_rate * 0.1
    )
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ (è®¾å¤‡: {config.device})...")
    print("-" * 60)
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    start_time = time.time()
    
    # è·å–æ•°æ®è¿­ä»£å™¨
    train_iter = iter(train_loader)
    
    # åˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(range(config.max_iters), desc="è®­ç»ƒè¿›åº¦", unit="step", ncols=120)
    
    for step in pbar:
        # è¯„ä¼°æ¨¡å‹
        if step % config.eval_interval == 0 or step == config.max_iters - 1:
            losses = estimate_loss(model, train_loader, val_loader, config.eval_iters, config.device)
            
            elapsed_time = time.time() - start_time
            print(f"\næ­¥æ•° {step:5d} | è®­ç»ƒæŸå¤±: {losses['train']:.4f} | éªŒè¯æŸå¤±: {losses['val']:.4f} | "
                  f"æ—¶é—´: {elapsed_time:.1f}s | å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
            
            train_losses.append(losses['train'].item())
            val_losses.append(losses['val'].item())
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if losses['val'] < best_val_loss:
                best_val_loss = losses['val']
                best_model_path = save_path.replace('.pth', '_best.pth')
                # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(best_model_path), exist_ok=True)
                model.save_pretrained(best_model_path, config)
        
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        try:
            xb, yb = next(train_iter)
        except StopIteration:
            train_iter = iter(train_loader)
            xb, yb = next(train_iter)
        
        xb, yb = xb.to(config.device), yb.to(config.device)
        
        # å‰å‘ä¼ æ’­
        logits, loss = model(xb, yb)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        scheduler.step()
        
        # æ›´æ–°è¿›åº¦æ¡æè¿°
        if step % 10 == 0:  # æ¯10æ­¥æ›´æ–°ä¸€æ¬¡è¿›åº¦æ¡ï¼Œé¿å…è¿‡äºé¢‘ç¹
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_description(f"è®­ç»ƒè¿›åº¦ | æŸå¤±: {loss.item():.4f} | å­¦ä¹ ç‡: {current_lr:.2e}")
    
    pbar.close()
    
    total_time = time.time() - start_time
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plot_training_curves(train_losses, val_losses, config)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save_pretrained(save_path, config)
    
    return model, tokenizer

def plot_training_curves(train_losses: list, val_losses: list, config: Config):
    """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
    
    Args:
        train_losses: è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses: éªŒè¯æŸå¤±åˆ—è¡¨
        config: é…ç½®å¯¹è±¡
    """
    print("\nğŸ“Š ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    
    plt.figure(figsize=(12, 5))
    
    # æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    steps = [i * config.eval_interval for i in range(len(train_losses))]
    plt.plot(steps, train_losses, label='Training loss', marker='o', linewidth=2)
    plt.plot(steps, val_losses, label='Validation loss', marker='s', linewidth=2)
    plt.xlabel('Training steps')
    plt.ylabel('Loss')
    plt.title('Loss during training')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æŸå¤±å¯¹æ•°å›¾
    plt.subplot(1, 2, 2)
    plt.semilogy(steps, train_losses, label='Training loss', marker='o', linewidth=2)
    plt.semilogy(steps, val_losses, label='Validation loss', marker='s', linewidth=2)
    plt.xlabel('Training steps')
    plt.ylabel('Loss (log scale)')
    plt.title('Loss during training (log scale)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨
    os.makedirs('../results', exist_ok=True)
    
    # ä¿å­˜å›¾ç‰‡åˆ°resultsæ–‡ä»¶å¤¹
    loss_plot_path = '../results/training_loss.png'
    plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º '{loss_plot_path}'")
    
    # æ˜¾ç¤ºå›¾ç‰‡ï¼ˆå¦‚æœåœ¨æ”¯æŒçš„ç¯å¢ƒä¸­ï¼‰
    try:
        plt.show()
    except:
        print("æ³¨æ„: æ— æ³•æ˜¾ç¤ºå›¾ç‰‡ï¼Œè¯·æŸ¥çœ‹ä¿å­˜çš„å›¾ç‰‡æ–‡ä»¶")

def demo_generation(model: TransformerLanguageModel, tokenizer, config: Config):
    """æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆ
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        config: é…ç½®å¯¹è±¡
    """
    print("\n" + "=" * 60)
    print("ğŸ² æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("=" * 60)
    
    model.eval()
    
    # é’ˆå¯¹ç§‘å¹»æ•°æ®çš„æç¤ºè¯
    prompts = [
        "",  # æ— æç¤º
        "The spaceship",
        "In the future",
        "The alien",
        "Technology",
        "The planet",
        "Captain"
    ]
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\n{i}. æç¤º: '{prompt}' {'(æ— æç¤º)' if not prompt else ''}")
        print("-" * 50)
        
        try:
            generated_text = generate_text(
                model=model,
                tokenizer=tokenizer, 
                prompt=prompt,
                max_new_tokens=50,
                temperature=0.8,
                device=config.device
            )
            
            # åªæ˜¾ç¤ºæ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰åŸå§‹æç¤ºï¼‰
            if prompt and generated_text.startswith(prompt):
                display_text = generated_text[len(prompt):].strip()
            else:
                display_text = generated_text
            
            print(f"{display_text}")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("æ¨¡å—åŒ–Transformerè¯­è¨€æ¨¡å‹ - è®­ç»ƒç¨‹åº")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # é’ˆå¯¹å¤§æ•°æ®é›†è°ƒæ•´é…ç½®
    config.batch_size = 8  # å¢åŠ æ‰¹æ¬¡å¤§å°
    config.context_length = 32  # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦
    config.max_iters = 10000  # å¢åŠ è®­ç»ƒæ­¥æ•°
    config.eval_interval = 1000  # è°ƒæ•´è¯„ä¼°é—´éš”
    
    print(f"å½“å‰é…ç½®:")
    print(f"  è®¾å¤‡: {config.device}")
    print(f"  æ¨¡å‹å‚æ•°: d_model={config.d_model}, heads={config.num_heads}, blocks={config.num_blocks}")
    print(f"  è®­ç»ƒå‚æ•°: batch_size={config.batch_size}, max_iters={config.max_iters}")
    print(f"  ä¸Šä¸‹æ–‡é•¿åº¦: {config.context_length}")
    
    # å¼€å§‹è®­ç»ƒ
    model, tokenizer = train_model(config)
    
    if model is not None and tokenizer is not None:
        # æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆ
        demo_generation(model, tokenizer, config)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è®­ç»ƒå’Œæ¼”ç¤ºå®Œæˆï¼")
        print("\nç”Ÿæˆçš„æ–‡ä»¶:")
        print("  - ../model/transformer_model.pth (æœ€ç»ˆæ¨¡å‹)")
        print("  - ../model/transformer_model_best.pth (æœ€ä½³æ¨¡å‹)")
        print("  - ../results/training_loss.png (è®­ç»ƒæ›²çº¿)")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python example_usage.py  # äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆ")
        print("=" * 60)
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥")

if __name__ == "__main__":
    main() 